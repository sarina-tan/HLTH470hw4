---
title: "Homework 4-2"
format: pdf
author: "Sarina Tan"
execute:
    echo: false
---
# The link to my repository: https://github.com/sarina-tan/HLTH470hw4/tree/main

{{< pagebreak >}}

```{python, echo: false}
# Import libraries
import pandas as pd
import numpy as np
import os
from sklearn.linear_model import LogisticRegression
from sklearn.utils import resample
from scipy.spatial.distance import mahalanobis
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt
import statsmodels.api as sm
import matplotlib.ticker as ticker
import matplotlib
import seaborn as sns
from tabulate import tabulate
from statsmodels.formula.api import ols
from linearmodels.iv import IV2SLS
from causalinference import CausalModel
from sklearn.neighbors import NearestNeighbors
from IPython.display import Markdown, display
import warnings
warnings.simplefilter('ignore')
warnings.simplefilter(action='ignore', category=pd.errors.DtypeWarning)
# Load data
df = pd.read_csv("/Users/sarinatan/Desktop/HLTH470hw4/data/output/final_ma_data.csv")
# create separate copy for question
df_q1 = df.copy()
```

# 1. Remove all SNPs, 800-series plans, and prescription drug only plans (i.e., plans that do not offer Part C benefits). Provide a box and whisker plot showing the distribution of plan counts by county over time. Do you think that the number of plans is sufficient, too few, or too many?

```{python, echo: false}
# Filter out SNPs, 800, drug only
filtered = df_q1[
    (df_q1['snp'] == 'No') &
    ((df_q1['planid'] < 800) | (df_q1['planid'] >= 900)) &
    (df_q1['plan_type'].notna())
].copy()

# Count plans per county per year
plan_counts = (
    filtered.groupby(["year", "fips"])
    .size()
    .reset_index(name="plan_count")
)

# Remove outliers using IQR method
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Apply outlier removal per year
plan_counts_no_outliers = (
    plan_counts.groupby('year', group_keys=False)
    .apply(lambda x: remove_outliers(x, 'plan_count'))
)

# Boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=plan_counts_no_outliers, x="year", y="plan_count")
plt.title("Distribution of Plan Counts per County by Year (Outliers Removed)")
plt.xlabel("Year")
plt.ylabel("Plan Count")
plt.grid(True)
plt.tight_layout()
plt.show()
```
####### Interpretation: Overall, I think that the number of plans is sufficient but the wide variation and outliers may imply both too few and too many. 

{{< pagebreak >}}

# 2. Provide bar graphs showing the distribution of star ratings in 2010, 2012, and 2015. How has this distribution changed over time?

```{python, echo: false}
filtered.loc[:, 'Star_Rating'] = pd.to_numeric(filtered['Star_Rating'], errors='coerce')
years = [2010, 2012, 2015]

# Create 1 row with 3 columns of subplots
fig, axes = plt.subplots(1, 3, figsize=(24, 6), sharey=True)

for i, year in enumerate(years):
    ax = axes[i]
    yearly_data = filtered[filtered['year'] == year]
    sns.countplot(
        data=yearly_data,
        x='Star_Rating',
        hue='Star_Rating',
        palette='crest',
        order=sorted(yearly_data['Star_Rating'].dropna().unique()),
        ax=ax,
        legend=False
    )
    ax.set_title(f"Distribution of Star Ratings in {year}", fontsize=16)
    ax.set_xlabel("Star Rating", fontsize=14)
    ax.set_ylabel("Number of Plans" if i == 0 else "", fontsize=14)
    ax.tick_params(axis='x', labelsize=12)
    ax.tick_params(axis='y', labelsize=12)

plt.tight_layout()
plt.show()
```

####### Interpretation: Over time, star ratings trended rightward over time, likely due to actual enhancements in plan quality, adjustments in CMS evaluation methods, or a combination of the two. By 2015, the market was largely composed of higher-rated plans, a shift that could significantly impact consumer decisions and the financial incentives linked to star ratings.
{{< pagebreak >}}

# 3. Plot the average benchmark payment over time from 2010 through 2015. How much has the average benchmark payment risen over the years?

```{python, echo: false}
# Ensure ma_rate is numeric
df["ma_rate"] = pd.to_numeric(df["ma_rate"], errors="coerce")

# Filter valid years and benchmark data
benchmark_trend = (
    df[(df['year'] >= 2010) & (df['year'] <= 2015) & (df['ma_rate'].notna())]
    .groupby("year", as_index=False)["ma_rate"]
    .mean()
)

# Plot average benchmark payment by year
plt.figure(figsize=(8, 5))
sns.lineplot(data=benchmark_trend, x="year", y="ma_rate", marker="o")
plt.title("Average Benchmark Payment (2010–2015)")
plt.xlabel("Year")
plt.ylabel("Average Benchmark Payment ($)")
plt.grid(True)
plt.tight_layout()
plt.show()
plt.close()

print("Average Benchmark Payment by Year:")
display(benchmark_trend)

growth = benchmark_trend['ma_rate'].iloc[-1] - benchmark_trend['ma_rate'].iloc[0]
print(f"\nIncrease from 2010 to 2015: ${growth:.2f}")
```
####### Interpretation: From 2010 to 2014, average benchmark payments for Medicare Advantage plans rose steadily, peaking at $836.00. However, in 2015, the benchmark dropped sharply to $782.71. Despite earlier increases, the overall change from 2010 to 2015 was a net decrease of $21.23, possibly due to policy changes or cost-containment measures.

{{< pagebreak >}}

# 4. Plot the average share of Medicare Advantage (relative to all Medicare eligibles) over time from 2010 through 2015. Has Medicare Advantage increased or decreased in popularity? How does this share correlate with benchmark payments?

```{python, echo: false}
# Convert to numeric just in case
df["enrolled_mean"] = pd.to_numeric(df["avg_enrolled"], errors="coerce")
df["eligibles_mean"] = pd.to_numeric(df["avg_eligibles"], errors="coerce")

# Calculate Medicare Advantage (MA) share per plan
df["ma_share"] = df["enrolled_mean"] / df["eligibles_mean"]

# Group by year to calculate average MA share across counties/plans
ma_share_trend = (
    df[(df["year"] >= 2010) & (df["year"] <= 2015) & (df["ma_share"].notna())]
    .groupby("year", as_index=False)["ma_share"]
    .mean()
)

# Merge MA share trend and benchmark trend
merged_trend = pd.merge(ma_share_trend, benchmark_trend, on="year")

# Create figure and primary y-axis
fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot MA share on primary axis
color1 = "tab:blue"
ax1.set_xlabel("Year")
ax1.set_ylabel("MA Share", color=color1)
ax1.plot(merged_trend["year"], merged_trend["ma_share"], marker="o", color=color1, label="MA Share")
ax1.tick_params(axis="y", labelcolor=color1)

# Create secondary y-axis for benchmark payment
ax2 = ax1.twinx()
color2 = "tab:green"
ax2.set_ylabel("Benchmark Payment ($)", color=color2)
ax2.plot(merged_trend["year"], merged_trend["ma_rate"], marker="o", linestyle="--", color=color2, label="Benchmark Payment")
ax2.tick_params(axis="y", labelcolor=color2)

# Titles and layout
plt.title("MA Share and Benchmark Payments (2010–2015)")
fig.tight_layout()
plt.show()
plt.close()

# Correlation
correlation = merged_trend["ma_share"].corr(merged_trend["ma_rate"])
print(f"\nCorrelation between MA share and benchmark payment: {correlation:.3f}")
```
####### Interpretation: The correlation between average benchmark payments and market shares is just 0.065, so it doesn't seem to be strongly related to Medicare Advantage benchmark payments.

{{< pagebreak >}}

# 5. Calculate the running variable underlying the star rating. Provide a table showing the number of plans that are rounded up into a 3-star, 3.5-star, 4-star, 4.5-star, and 5-star rating.

```{python, echo: false}
# Filter to 2010 only
data_2010 = df[df["year"] == 2010].copy()

# Define raw Star Rating based on Part D rules
data_2010["Star_Rating"] = np.where(
    data_2010["partd"] == "No",
    data_2010["partc_score"],
    np.where(
        data_2010["partcd_score"].isna(),
        data_2010["partc_score"],
        data_2010["partcd_score"]
    )
)

# Create rounded rating to nearest 0.5
data_2010["rounded_rating"] = np.round(data_2010["Star_Rating"] * 2) / 2

# Count number of plans by rounded rating
rating_counts = data_2010["rounded_rating"].value_counts().sort_index()

# Convert to DataFrame for display
rating_counts_df = rating_counts.reset_index()
rating_counts_df.columns = ["Rounded Star Rating", "Number of Plans"]

# Show table
print("Number of Plans by Rounded Star Rating (2010):")
display(rating_counts_df)
```
{{< pagebreak >}}

# 6. Using the RD estimator with a bandwidth of 0.125, provide an estimate of the effect of receiving a 3-star versus a 2.5 star rating on enrollments. Repeat the exercise to estimate the effects at 3.5 stars, and summarize your results in a table.
```{python, echo: false}
# STEP 1: Setup data_2010 and raw_rating
data_2010 = df[df["year"] == 2010].copy()

data_2010["raw_rating"] = np.where(
    data_2010["partd"] == "No",
    data_2010["partc_score"],
    np.where(data_2010["partcd_score"].isna(), data_2010["partc_score"], data_2010["partcd_score"])
)

data_2010["avg_enrolled"] = pd.to_numeric(data_2010["avg_enrollment"], errors="coerce")

# STEP 2: RD Estimation Function
def rd_estimate(data, cutoff, bandwidth=0.125):
    rd_data = data[
        (data["raw_rating"] >= cutoff - bandwidth) &
        (data["raw_rating"] <= cutoff + bandwidth)
    ].copy()

    rd_data["treatment"] = (rd_data["raw_rating"] >= cutoff).astype(int)
    rd_data = rd_data.dropna(subset=["avg_enrolled"])

    X = sm.add_constant(rd_data["treatment"])
    y = rd_data["avg_enrolled"]
    model = sm.OLS(y, X).fit()

    return {
        "Cutoff": cutoff,
        "Bandwidth": bandwidth,
        "Estimated ATE": model.params["treatment"],
        "Standard Error": model.bse["treatment"],
        "N": len(rd_data)
    }

# STEP 3: Run estimates
results = [
    rd_estimate(data_2010, cutoff=3.0, bandwidth=0.125),
    rd_estimate(data_2010, cutoff=3.5, bandwidth=0.125)
]

summary_df = pd.DataFrame(results)
display(summary_df)
```
{{< pagebreak >}}

# 7. Repeat your results for bandwidhts of 0.1, 0.12, 0.13, 0.14, and 0.15 (again for 3 and 3.5 stars). Show all of the results in a graph. How sensitive are your findings to the choice of bandwidth?

```{python, echo: false}
# --- Step 1: Reuse the rd_estimate function ---
def rd_estimate(data, cutoff, bandwidth=0.125):
    rd_data = data[
        (data["raw_rating"] >= cutoff - bandwidth) &
        (data["raw_rating"] <= cutoff + bandwidth)
    ].copy()

    rd_data["treatment"] = (rd_data["raw_rating"] >= cutoff).astype(int)
    rd_data = rd_data.dropna(subset=["avg_enrolled"])

    X = sm.add_constant(rd_data["treatment"])
    y = rd_data["avg_enrolled"]
    model = sm.OLS(y, X).fit()

    return model.params["treatment"], model.bse["treatment"]

# --- Step 2: Run RD for multiple bandwidths and cutoffs ---
bandwidths = [0.1, 0.12, 0.13, 0.14, 0.15]
results = []

for bw in bandwidths:
    for cutoff in [3.0, 3.5]:
        ate, se = rd_estimate(data_2010, cutoff=cutoff, bandwidth=bw)
        results.append({
            "Cutoff": cutoff,
            "Bandwidth": bw,
            "ATE": ate,
            "SE": se
        })

# --- Step 3: Convert to DataFrame ---
rd_results = pd.DataFrame(results)

# --- Step 4: Plot ATEs with error bars ---
fig, ax = plt.subplots(figsize=(10, 6))

for cutoff, color in zip([3.0, 3.5], ['blue', 'green']):
    subset = rd_results[rd_results["Cutoff"] == cutoff]
    ax.errorbar(
        subset["Bandwidth"], subset["ATE"], yerr=subset["SE"],
        label=f'Cutoff {cutoff}', marker='o', linestyle='-', color=color, capsize=5
    )

# --- Step 5: Final Styling ---
ax.set_title("RD Estimates of Star Rating on Enrollment\nSensitivity to Bandwidth", fontsize=14)
ax.set_xlabel("Bandwidth")
ax.set_ylabel("Estimated ATE on Enrollment")
ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)
ax.legend()
plt.xticks(bandwidths)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```
{{< pagebreak >}}

# 8. Examine (graphically) whether contracts appear to manipulate the running variable. In other words, look at the distribution of the running variable before and after the relevent threshold values. What do you find?

```{python, echo: false}
# Load and prep the data
df['partc_score'] = pd.to_numeric(df['partc_score'], errors='coerce')
df['partcd_score'] = pd.to_numeric(df['partcd_score'], errors='coerce')

# Focus on 2010
rdd_data = df[df['year'] == 2010].copy()

# Define running variable
rdd_data['running_score'] = np.where(
    rdd_data['partd'] == 'No',
    rdd_data['partc_score'],
    rdd_data['partcd_score']
)

# Keep only plans with valid running scores
rdd_data = rdd_data[rdd_data['running_score'].notna()]

# Function to plot density around cutoff
def plot_density(data, cutoff, bandwidth=0.3):
    window = data[
        (data['running_score'] >= cutoff - bandwidth) &
        (data['running_score'] <= cutoff + bandwidth)
    ]

    plt.figure(figsize=(8, 5))
    sns.histplot(window['running_score'], bins=40, kde=False, color="slateblue", edgecolor="white")
    plt.axvline(cutoff, color='red', linestyle='--', label=f"Cutoff = {cutoff}")
    plt.title(f"Density of Running Variable Near Cutoff = {cutoff}")
    plt.xlabel("Running Score")
    plt.ylabel("Number of Plans")
    plt.legend()
    plt.tight_layout()
    plt.show()
    plt.close()

# Run for cutoffs 3.0 and 3.5
plot_density(rdd_data, cutoff=3.0)
plot_density(rdd_data, cutoff=3.5)
```
####### Interpretation: To evaluate potential manipulation of star ratings around key thresholds (3.0 and 3.5), the distribution of the raw running variable was analyzed within ±0.3 of each cutoff. The resulting histograms show smooth, continuous distributions with no sharp increases or clustering just above the thresholds. These patterns suggest no clear evidence of manipulation, indicating that plans do not appear to be strategically adjusting ratings to cross these benchmarks.
{{< pagebreak >}}

# 9. Similar to question 4, examine whether plans just above the threshold values have different characteristics than contracts just below the threshold values. Use HMO and Part D status as your plan characteristics.

```{python, echo: false}
# Focus on 2010
data_2010 = df[df["year"] == 2010].copy()

# If not already present, calculate raw rating (same logic used in earlier parts)
data_2010["raw_rating"] = np.where(
    data_2010["partd"] == "No",
    data_2010["partc_score"],
    np.where(data_2010["partcd_score"].isna(), data_2010["partc_score"], data_2010["partcd_score"])
)

# Function to summarize plan characteristics above/below RD cutoff
def covariate_check(data, cutoff, bandwidth):
    subset = data[
        (data["raw_rating"] >= cutoff - bandwidth) &
        (data["raw_rating"] <= cutoff + bandwidth)
    ].copy()
    
    subset["above"] = (subset["raw_rating"] >= cutoff).astype(int)

    summary = subset.groupby("above").agg(
        share_hmo=("plan_type", lambda x: (x.str.upper() == "HMO").mean()),
        share_partd=("partd", lambda x: (x == "Yes").mean()),
        n_plans=("contractid", "count")
    ).reset_index()

    summary["group"] = summary["above"].map({0: f"Below {cutoff}", 1: f"Above {cutoff}"})
    return summary[["group", "share_hmo", "share_partd", "n_plans"]]

# Run checks for both cutoffs
cov_3 = covariate_check(data_2010, cutoff=3.0, bandwidth=0.125)
cov_35 = covariate_check(data_2010, cutoff=3.5, bandwidth=0.125)

# Combine into one table
covariate_table = pd.concat([cov_3, cov_35], ignore_index=True)
covariate_table.columns = ["Group", "Share HMO", "Share Part D", "Number of Plans"]

# Display the table
print("\nCovariate Balance Around RD Thresholds (Bandwidth = 0.125)")
display(covariate_table)
```
{{< pagebreak >}}

# 10. Summarize your findings from 5-9. What is the effect of increasing a star rating on enrollments? Briefly explain your results.

```{python, echo: false}
# Question 5: Rounded Star Ratings (2010)
rating_counts_df = pd.DataFrame({
    "Rounded Star Rating": [2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0],
    "Number of Plans": [431, 27549, 7419, 6347, 5453, 2459, 75]
})

# Question 6: RD Estimates at Cutoffs 3.0 and 3.5
rd_estimates_df = pd.DataFrame({
    "Cutoff": [3.0, 3.5],
    "Bandwidth": [0.125, 0.125],
    "Estimated ATE": [740.394199, 418.940257],
    "Standard Error": [47.231624, 23.056193],
    "N (Sample Size)": [3034, 2879]
})

# Question 9: Covariate Balance Around RD Thresholds
covariate_balance_df = pd.DataFrame({
    "Group": ["Above 3.0", "Above 3.5"],
    "Share HMO": [0.0, 0.0],
    "Share Part D": [0.893651, 0.853317],
    "Number of Plans": [7419, 6347]
})

# Merge all into one summary table
summary_combined = pd.DataFrame({
    "Cutoff": [3.0, 3.5],
    "Rounded Star Rating Plan Count": [
        rating_counts_df[rating_counts_df["Rounded Star Rating"] == 3.0]["Number of Plans"].values[0],
        rating_counts_df[rating_counts_df["Rounded Star Rating"] == 3.5]["Number of Plans"].values[0]
    ],
    "Estimated ATE": rd_estimates_df["Estimated ATE"],
    "Standard Error": rd_estimates_df["Standard Error"],
    "N (Sample Size)": rd_estimates_df["N (Sample Size)"],
    "Share HMO": covariate_balance_df["Share HMO"],
    "Share Part D": covariate_balance_df["Share Part D"]
})

display(summary_combined)
```
######### Interpretation: Summary of Findings from Questions 5–9
The analysis shows that increases in star ratings have a significant positive impact on plan enrollment. At the 3.0-star cutoff, crossing the threshold results in about 740 additional enrollees (SE ≈ 47), while at the 3.5-star cutoff, the increase is around 419 enrollees (SE ≈ 23). These estimates come from a regression discontinuity design (RDD), leveraging the near-random assignment of plans around each threshold.

